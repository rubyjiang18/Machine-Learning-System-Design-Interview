## Chapter 1: Introduction and Overview

- [x] [Data warehouse](https://cloud.google.com/learn/what-is-a-data-warehouse)
- [ ] [Structure vs. unstructured data]()
- [ ] [Bagging technique in ensemble learning]()
- [ ] [Boosting technique in ensemble learning]()
- [ ] [Stacking technique in ensemle learning]()
- [ ] [Interpretability in Machine Learning]()
- [ ] [Traditional machine learning algorithms]()
- [ ] [Sampling strategies](https://www.scribbr.com/methodology/sampling-methods/)
- [ ] [Data splitting rechniques]()
- [ ] [Class-balanced loss]()
- [ ] [Focal loss papar]()
- [ ] [Focal loss]()
- [ ] [Data parammelism](https://www.telesens.co/2017/12/25/understanding-data-parallelism-in-machine-learning/)
- [ ] [Model parallelism]()
- [ ] [Cross entropy loss]()
- [ ] [Mean squared error loss]()
- [ ] [Mean absolute error loss]()
- [ ] [L1 and L2 regularization]()
- [ ] [Entropy regularization]()
- [ ] [K-fold cross validation]()
- [ ] [Dropout paper]()
- [ ] [Overview of optimization algorithm]()
- [ ] [Stochastic gradient descent]()
- [ ] [AdaGrad optimization algorithm]()
- [ ] [Momentum optimization algorithm]()
- [ ] [RMSProp optimization algorithm]()
- [ ] [ELU activation function]()
- [ ] [ReLU activation function]()
- [ ] [Tanh activation function]()
- [ ] [Sigmoid activation function]()
- [ ] [FID score]()
- [ ] [Inception score]()
- [ ] [BLEU score]()
- [ ] [METEOR score]()
- [ ] [ROUGE score]()
- [ ] [CIDEr score]()
- [ ] [SPICE score]()
- [ ] [Quantization-aware training](https://pytorch.org/docs/stable/quantization.html)
- [ ] [Model compression survey]()
- [ ] [Shadow deployment]()
- [ ] [A/B testing]()
- [ ] [Canary release]()
- [ ] [Interleaving experiment]()
- [ ] [Multi-armed bandit]()
- [ ] [ML infrastructure]()
- [ ] [Interpretability in ML]()
- [x] Chip Huyen. Designing Machine Learning Systems: An Iterative Process for Production-Ready Application. O'Reilly Media, Inc., 2022